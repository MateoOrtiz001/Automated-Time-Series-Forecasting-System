"""
An√°lisis de variables ex√≥genas para predicci√≥n de inflaci√≥n.

Este script eval√∫a qu√© variables tienen mayor correlaci√≥n/importancia
con respecto a la inflaci√≥n y recomienda cu√°les mantener o descartar.
"""

import sys
from pathlib import Path
from typing import List, Tuple

# Agregar ra√≠z del proyecto al path ANTES de otros imports
ROOT_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT_DIR))

# Importar TFTModel primero para que TensorFlow se cargue antes de sklearn
from src.model.model import TFTModel

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Backend no interactivo
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Importar sklearn DESPU√âS de TensorFlow
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import mutual_info_regression

# Configuraci√≥n
PROC_DIR = ROOT_DIR / "data" / "proc"
RESULTS_DIR = ROOT_DIR / "results"
TARGET_COL = "Inflacion_total"
MAX_LAG = 12  # M√°ximo lag para an√°lisis de correlaci√≥n cruzada

# Descripciones de variables para interpretaci√≥n
VARIABLE_DESCRIPTIONS = {
    "Inflacion_total": "Inflaci√≥n total (variaci√≥n anual %)",
    "IPP": "√çndice de Precios del Productor (base dic/2014=100)",
    "PIB_real_trimestral_2015_AE": "PIB real trimestral con ajuste estacional (miles de millones COP)",
    "Tasa_interes_colocacion_total": "Tasa de inter√©s de colocaci√≥n total (% efectiva anual)",
    "TRM": "Tasa Representativa del Mercado (COP/USD)",
    "Brent": "Precio del petr√≥leo Brent (USD/barril)",
    "FAO": "√çndice de precios de alimentos FAO (base 2014-2016=100)",
}


def load_data() -> pd.DataFrame:
    """Carga el dataset procesado m√°s reciente."""
    df = TFTModel.load_latest_proc_csv(PROC_DIR)
    df = df.dropna().sort_values("date").reset_index(drop=True)
    return df


def get_exog_cols(df: pd.DataFrame, target_col: str) -> List[str]:
    """Obtiene las columnas ex√≥genas (todas menos date y target)."""
    return [c for c in df.columns if c not in ["date", target_col]]


# =============================================================================
# 1. CORRELACI√ìN DE PEARSON (CONTEMPOR√ÅNEA)
# =============================================================================
def analyze_pearson_correlation(df: pd.DataFrame, target_col: str, exog_cols: List[str]) -> pd.DataFrame:
    """Calcula correlaci√≥n de Pearson entre ex√≥genas y target."""
    results = []
    for col in exog_cols:
        corr, pvalue = stats.pearsonr(df[col], df[target_col])
        results.append({
            "Variable": col,
            "Correlaci√≥n Pearson": corr,
            "P-value": pvalue,
            "Significativa (p<0.05)": "S√≠" if pvalue < 0.05 else "No",
            "|Correlaci√≥n|": abs(corr),
        })
    
    results_df = pd.DataFrame(results).sort_values("|Correlaci√≥n|", ascending=False)
    return results_df


# =============================================================================
# 2. CORRELACI√ìN CRUZADA (CON LAGS)
# =============================================================================
def analyze_cross_correlation(df: pd.DataFrame, target_col: str, exog_cols: List[str], max_lag: int = 12) -> pd.DataFrame:
    """Calcula correlaci√≥n cruzada con diferentes lags."""
    results = []
    
    for col in exog_cols:
        best_corr = 0
        best_lag = 0
        
        for lag in range(-max_lag, max_lag + 1):
            if lag < 0:
                # Variable ex√≥gena adelantada (target rezagado)
                x = df[col].iloc[:lag].values
                y = df[target_col].iloc[-lag:].values
            elif lag > 0:
                # Variable ex√≥gena rezagada (target adelantado)
                x = df[col].iloc[lag:].values
                y = df[target_col].iloc[:-lag].values
            else:
                x = df[col].values
                y = df[target_col].values
            
            if len(x) > 10:
                corr, _ = stats.pearsonr(x, y)
                if abs(corr) > abs(best_corr):
                    best_corr = corr
                    best_lag = lag
        
        results.append({
            "Variable": col,
            "Mejor Correlaci√≥n": best_corr,
            "Mejor Lag": best_lag,
            "|Mejor Correlaci√≥n|": abs(best_corr),
            "Interpretaci√≥n Lag": "Variable anticipa target" if best_lag > 0 else ("Target anticipa variable" if best_lag < 0 else "Contempor√°nea"),
        })
    
    results_df = pd.DataFrame(results).sort_values("|Mejor Correlaci√≥n|", ascending=False)
    return results_df


# =============================================================================
# 3. TEST DE CAUSALIDAD DE GRANGER (Simplificado)
# =============================================================================
def analyze_granger_causality(df: pd.DataFrame, target_col: str, exog_cols: List[str], max_lag: int = 6) -> pd.DataFrame:
    """
    Realiza test de causalidad de Granger simplificado.
    En lugar de regresiones complejas, usa correlaci√≥n de cambios.
    """
    
    def manual_correlation(x, y):
        """Correlaci√≥n de Pearson calculada manualmente."""
        n = len(x)
        if n < 3:
            return 0.0
        mean_x = sum(x) / n
        mean_y = sum(y) / n
        
        num = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))
        den_x = sum((xi - mean_x) ** 2 for xi in x) ** 0.5
        den_y = sum((yi - mean_y) ** 2 for yi in y) ** 0.5
        
        if den_x == 0 or den_y == 0:
            return 0.0
        return num / (den_x * den_y)
    
    results = []
    
    target_series = list(df[target_col].values)
    target_diff = [target_series[i+1] - target_series[i] for i in range(len(target_series)-1)]
    
    for col in exog_cols:
        exog_series = list(df[col].values)
        exog_diff = [exog_series[i+1] - exog_series[i] for i in range(len(exog_series)-1)]
        
        # Buscar el mejor lag donde los cambios de exog anticipan cambios de target
        best_corr = 0.0
        best_lag = 1
        
        for lag in range(1, min(max_lag + 1, len(target_diff) - 10)):
            # exog_diff[:-lag] anticipa target_diff[lag:]
            x = exog_diff[:-lag]
            y = target_diff[lag:]
            
            if len(x) > 10 and len(x) == len(y):
                corr = manual_correlation(x, y)
                if abs(corr) > abs(best_corr):
                    best_corr = corr
                    best_lag = lag
        
        # Considerar causalidad si hay correlaci√≥n significativa
        granger_yes = abs(best_corr) > 0.15
        
        results.append({
            "Variable": col,
            "Granger-causa Target": "S√≠" if granger_yes else "No",
            "Correlaci√≥n Œî": round(best_corr, 4),
            "Lag √≥ptimo": best_lag,
        })
    
    results_df = pd.DataFrame(results)
    # Ordenar por valor absoluto de correlaci√≥n manualmente
    results_df["_abs_corr"] = results_df["Correlaci√≥n Œî"].abs()
    results_df = results_df.sort_values("_abs_corr", ascending=False).drop(columns=["_abs_corr"])
    return results_df


# =============================================================================
# 4. INFORMACI√ìN MUTUA
# =============================================================================
def analyze_mutual_information(df: pd.DataFrame, target_col: str, exog_cols: List[str]) -> pd.DataFrame:
    """Calcula informaci√≥n mutua entre ex√≥genas y target."""
    X = df[exog_cols].values
    y = df[target_col].values
    
    # Estandarizar
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Calcular MI
    mi_scores = mutual_info_regression(X_scaled, y, random_state=42)
    
    results = pd.DataFrame({
        "Variable": exog_cols,
        "Informaci√≥n Mutua": mi_scores,
    }).sort_values("Informaci√≥n Mutua", ascending=False)
    
    # Normalizar a porcentaje
    results["MI Normalizada (%)"] = 100 * results["Informaci√≥n Mutua"] / results["Informaci√≥n Mutua"].sum()
    
    return results


# =============================================================================
# 5. IMPORTANCIA DE FEATURES (RANDOM FOREST)
# =============================================================================
def analyze_feature_importance_rf(df: pd.DataFrame, target_col: str, exog_cols: List[str]) -> pd.DataFrame:
    """Calcula importancia de features usando Random Forest."""
    X = df[exog_cols].values
    y = df[target_col].values
    
    # Estandarizar
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Entrenar Random Forest
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_scaled, y)
    
    results = pd.DataFrame({
        "Variable": exog_cols,
        "Importancia RF": rf.feature_importances_,
    }).sort_values("Importancia RF", ascending=False)
    
    results["Importancia RF (%)"] = 100 * results["Importancia RF"] / results["Importancia RF"].sum()
    
    return results


# =============================================================================
# 6. IMPORTANCIA DE FEATURES (GRADIENT BOOSTING)
# =============================================================================
def analyze_feature_importance_gb(df: pd.DataFrame, target_col: str, exog_cols: List[str]) -> pd.DataFrame:
    """Calcula importancia de features usando Gradient Boosting."""
    X = df[exog_cols].values
    y = df[target_col].values
    
    # Estandarizar
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Entrenar Gradient Boosting
    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
    gb.fit(X_scaled, y)
    
    results = pd.DataFrame({
        "Variable": exog_cols,
        "Importancia GB": gb.feature_importances_,
    }).sort_values("Importancia GB", ascending=False)
    
    results["Importancia GB (%)"] = 100 * results["Importancia GB"] / results["Importancia GB"].sum()
    
    return results


# =============================================================================
# 7. AN√ÅLISIS CONSOLIDADO Y RECOMENDACIONES
# =============================================================================
def consolidate_analysis(
    pearson_df: pd.DataFrame,
    cross_corr_df: pd.DataFrame,
    granger_df: pd.DataFrame,
    mi_df: pd.DataFrame,
    rf_df: pd.DataFrame,
    gb_df: pd.DataFrame,
) -> pd.DataFrame:
    """Consolida todos los an√°lisis en un ranking √∫nico."""
    
    # Merge de todos los resultados
    consolidated = pearson_df[["Variable", "|Correlaci√≥n|"]].copy()
    consolidated = consolidated.merge(
        cross_corr_df[["Variable", "|Mejor Correlaci√≥n|", "Mejor Lag"]], on="Variable"
    )
    consolidated = consolidated.merge(
        granger_df[["Variable", "Granger-causa Target", "Correlaci√≥n Œî"]], on="Variable"
    )
    consolidated = consolidated.merge(
        mi_df[["Variable", "MI Normalizada (%)"]], on="Variable"
    )
    consolidated = consolidated.merge(
        rf_df[["Variable", "Importancia RF (%)"]], on="Variable"
    )
    consolidated = consolidated.merge(
        gb_df[["Variable", "Importancia GB (%)"]], on="Variable"
    )
    
    # Calcular score compuesto (promedio de rankings normalizados)
    # Normalizar cada m√©trica a [0, 1]
    consolidated["Score Correlaci√≥n"] = consolidated["|Correlaci√≥n|"] / consolidated["|Correlaci√≥n|"].max()
    consolidated["Score Cross-Corr"] = consolidated["|Mejor Correlaci√≥n|"] / consolidated["|Mejor Correlaci√≥n|"].max()
    # Para Granger, usar el valor absoluto de la correlaci√≥n de cambios
    consolidated["Score Granger"] = consolidated["Correlaci√≥n Œî"].abs() / consolidated["Correlaci√≥n Œî"].abs().max()
    consolidated["Score MI"] = consolidated["MI Normalizada (%)"] / consolidated["MI Normalizada (%)"].max()
    consolidated["Score RF"] = consolidated["Importancia RF (%)"] / consolidated["Importancia RF (%)"].max()
    consolidated["Score GB"] = consolidated["Importancia GB (%)"] / consolidated["Importancia GB (%)"].max()
    
    # Score final (promedio)
    score_cols = ["Score Correlaci√≥n", "Score Cross-Corr", "Score Granger", "Score MI", "Score RF", "Score GB"]
    consolidated["SCORE FINAL"] = consolidated[score_cols].mean(axis=1)
    
    # Ordenar por score final
    consolidated = consolidated.sort_values("SCORE FINAL", ascending=False)
    
    return consolidated


def generate_recommendations(consolidated_df: pd.DataFrame, threshold: float = 0.3) -> Tuple[List[str], List[str]]:
    """Genera recomendaciones de qu√© variables mantener/descartar."""
    keep = consolidated_df[consolidated_df["SCORE FINAL"] >= threshold]["Variable"].tolist()
    discard = consolidated_df[consolidated_df["SCORE FINAL"] < threshold]["Variable"].tolist()
    return keep, discard


# =============================================================================
# VISUALIZACIONES
# =============================================================================
def plot_correlation_matrix(df: pd.DataFrame, target_col: str, exog_cols: List[str], save_path: Path):
    """Genera matriz de correlaci√≥n."""
    cols = [target_col] + exog_cols
    corr_matrix = df[cols].corr()
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, cmap="RdBu_r", center=0, fmt=".2f",
                square=True, linewidths=0.5)
    plt.title("Matriz de Correlaci√≥n: Inflaci√≥n vs Variables Ex√≥genas")
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    plt.close()


def plot_feature_importance(consolidated_df: pd.DataFrame, save_path: Path):
    """Gr√°fico de barras con scores de importancia."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Score Final
    ax1 = axes[0, 0]
    data = consolidated_df.sort_values("SCORE FINAL", ascending=True)
    ax1.barh(data["Variable"], data["SCORE FINAL"], color="steelblue")
    ax1.set_xlabel("Score Final")
    ax1.set_title("Score Final (Combinado)")
    ax1.axvline(x=0.3, color="red", linestyle="--", label="Umbral recomendado (0.3)")
    ax1.legend()
    
    # 2. Correlaci√≥n Pearson
    ax2 = axes[0, 1]
    data = consolidated_df.sort_values("|Correlaci√≥n|", ascending=True)
    ax2.barh(data["Variable"], data["|Correlaci√≥n|"], color="coral")
    ax2.set_xlabel("|Correlaci√≥n Pearson|")
    ax2.set_title("Correlaci√≥n con Inflaci√≥n")
    
    # 3. Importancia RF
    ax3 = axes[1, 0]
    data = consolidated_df.sort_values("Importancia RF (%)", ascending=True)
    ax3.barh(data["Variable"], data["Importancia RF (%)"], color="forestgreen")
    ax3.set_xlabel("Importancia (%)")
    ax3.set_title("Importancia Random Forest")
    
    # 4. Informaci√≥n Mutua
    ax4 = axes[1, 1]
    data = consolidated_df.sort_values("MI Normalizada (%)", ascending=True)
    ax4.barh(data["Variable"], data["MI Normalizada (%)"], color="darkorange")
    ax4.set_xlabel("Informaci√≥n Mutua (%)")
    ax4.set_title("Informaci√≥n Mutua")
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    plt.close()


def plot_time_series_comparison(df: pd.DataFrame, target_col: str, exog_cols: List[str], save_path: Path):
    """Gr√°fico de series temporales normalizadas."""
    fig, ax = plt.subplots(figsize=(16, 8))
    
    # Normalizar para visualizaci√≥n
    scaler = StandardScaler()
    normalized = pd.DataFrame(
        scaler.fit_transform(df[[target_col] + exog_cols]),
        columns=[target_col] + exog_cols
    )
    normalized["date"] = df["date"].values
    
    # Graficar target m√°s grueso
    ax.plot(normalized["date"], normalized[target_col], label=target_col, linewidth=3, color="black")
    
    # Graficar ex√≥genas
    colors = plt.cm.tab10(np.linspace(0, 1, len(exog_cols)))
    for col, color in zip(exog_cols, colors):
        ax.plot(normalized["date"], normalized[col], label=col, alpha=0.7, color=color)
    
    ax.set_xlabel("Fecha")
    ax.set_ylabel("Valor Normalizado (Z-score)")
    ax.set_title("Series Temporales Normalizadas: Inflaci√≥n vs Ex√≥genas")
    ax.legend(loc="upper left", bbox_to_anchor=(1, 1))
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    plt.close()


# =============================================================================
# MAIN
# =============================================================================
def main():
    print("=" * 70)
    print("AN√ÅLISIS DE VARIABLES EX√ìGENAS PARA PREDICCI√ìN DE INFLACI√ìN")
    print("=" * 70)
    
    # Cargar datos
    df = load_data()
    exog_cols = get_exog_cols(df, TARGET_COL)
    
    print(f"\nDataset: {len(df)} filas")
    print(f"Rango: {df['date'].min()} ‚Üí {df['date'].max()}")
    print(f"Target: {TARGET_COL}")
    print(f"Variables ex√≥genas ({len(exog_cols)}):")
    for col in exog_cols:
        desc = VARIABLE_DESCRIPTIONS.get(col, col)
        print(f"   ‚Ä¢ {col}: {desc}")
    
    # Estad√≠sticas b√°sicas del target
    print(f"\n{'='*70}")
    print("ESTAD√çSTICAS DEL TARGET (Inflaci√≥n)")
    print("="*70)
    print(df[TARGET_COL].describe())
    
    # 1. Correlaci√≥n de Pearson
    print(f"\n{'='*70}")
    print("1. CORRELACI√ìN DE PEARSON (CONTEMPOR√ÅNEA)")
    print("="*70)
    pearson_df = analyze_pearson_correlation(df, TARGET_COL, exog_cols)
    print(pearson_df.to_string(index=False))
    
    # 2. Correlaci√≥n cruzada
    print(f"\n{'='*70}")
    print("2. CORRELACI√ìN CRUZADA (CON LAGS)")
    print("="*70)
    cross_corr_df = analyze_cross_correlation(df, TARGET_COL, exog_cols, MAX_LAG)
    print(cross_corr_df.to_string(index=False))
    
    # 3. Causalidad de Granger (Simplificado - sin statsmodels)
    print(f"\n{'='*70}")
    print("3. TEST DE CAUSALIDAD DE GRANGER (Simplificado)")
    print("="*70)
    sys.stdout.flush()
    granger_df = analyze_granger_causality(df, TARGET_COL, exog_cols)
    sys.stdout.flush()
    print(granger_df.to_string(index=False))
    sys.stdout.flush()
    
    # 4. Informaci√≥n Mutua
    print(f"\n{'='*70}")
    print("4. INFORMACI√ìN MUTUA")
    print("="*70)
    mi_df = analyze_mutual_information(df, TARGET_COL, exog_cols)
    print(mi_df.to_string(index=False))
    
    # 5. Importancia Random Forest
    print(f"\n{'='*70}")
    print("5. IMPORTANCIA DE FEATURES (RANDOM FOREST)")
    print("="*70)
    rf_df = analyze_feature_importance_rf(df, TARGET_COL, exog_cols)
    print(rf_df.to_string(index=False))
    
    # 6. Importancia Gradient Boosting
    print(f"\n{'='*70}")
    print("6. IMPORTANCIA DE FEATURES (GRADIENT BOOSTING)")
    print("="*70)
    gb_df = analyze_feature_importance_gb(df, TARGET_COL, exog_cols)
    print(gb_df.to_string(index=False))
    
    # 7. An√°lisis consolidado
    print(f"\n{'='*70}")
    print("7. AN√ÅLISIS CONSOLIDADO Y RANKING FINAL")
    print("="*70)
    consolidated_df = consolidate_analysis(pearson_df, cross_corr_df, granger_df, mi_df, rf_df, gb_df)
    
    # Mostrar tabla resumida
    display_cols = ["Variable", "|Correlaci√≥n|", "|Mejor Correlaci√≥n|", "Mejor Lag",
                    "Granger-causa Target", "MI Normalizada (%)", 
                    "Importancia RF (%)", "Importancia GB (%)", "SCORE FINAL"]
    print(consolidated_df[display_cols].to_string(index=False))
    
    # 8. Recomendaciones
    print(f"\n{'='*70}")
    print("8. RECOMENDACIONES")
    print("="*70)
    
    keep, discard = generate_recommendations(consolidated_df, threshold=0.3)
    
    print("\n‚úÖ VARIABLES RECOMENDADAS PARA MANTENER (Score >= 0.3):")
    for i, var in enumerate(keep, 1):
        score = consolidated_df[consolidated_df["Variable"] == var]["SCORE FINAL"].values[0]
        print(f"   {i}. {var} (Score: {score:.3f})")
    
    print("\n‚ö†Ô∏è VARIABLES CANDIDATAS A DESCARTAR (Score < 0.3):")
    if discard:
        for i, var in enumerate(discard, 1):
            score = consolidated_df[consolidated_df["Variable"] == var]["SCORE FINAL"].values[0]
            print(f"   {i}. {var} (Score: {score:.3f})")
    else:
        print("   Ninguna - todas las variables tienen score >= 0.3")
    
    # 9. Interpretaci√≥n detallada
    print(f"\n{'='*70}")
    print("9. INTERPRETACI√ìN DETALLADA")
    print("="*70)
    
    for _, row in consolidated_df.iterrows():
        var = row["Variable"]
        desc = VARIABLE_DESCRIPTIONS.get(var, "")
        print(f"\nüìä {var}")
        if desc:
            print(f"   ({desc})")
        print(f"   ‚Ä¢ Correlaci√≥n Pearson: {row['|Correlaci√≥n|']:.3f}")
        print(f"   ‚Ä¢ Mejor correlaci√≥n cruzada: {row['|Mejor Correlaci√≥n|']:.3f} (lag={int(row['Mejor Lag'])} meses)")
        lag_interp = "anticipa" if row['Mejor Lag'] > 0 else ("sigue a" if row['Mejor Lag'] < 0 else "contempor√°nea con")
        if row['Mejor Lag'] != 0:
            print(f"     ‚Üí Esta variable {lag_interp} la inflaci√≥n por {abs(int(row['Mejor Lag']))} mes(es)")
        print(f"   ‚Ä¢ Granger-causa inflaci√≥n: {row['Granger-causa Target']}")
        print(f"   ‚Ä¢ Informaci√≥n Mutua: {row['MI Normalizada (%)']:.1f}%")
        print(f"   ‚Ä¢ Importancia RF: {row['Importancia RF (%)']:.1f}%")
        print(f"   ‚Ä¢ Importancia GB: {row['Importancia GB (%)']:.1f}%")
        print(f"   ‚Üí SCORE FINAL: {row['SCORE FINAL']:.3f}")
    
    # 10. Guardar resultados
    print(f"\n{'='*70}")
    print("10. GUARDANDO RESULTADOS")
    print("="*70)
    
    # CSV con an√°lisis consolidado
    output_csv = RESULTS_DIR / "feature_analysis_results.csv"
    consolidated_df.to_csv(output_csv, index=False)
    print(f"‚úì An√°lisis guardado en: {output_csv}")
    
    # Gr√°ficos
    plot_correlation_matrix(df, TARGET_COL, exog_cols, RESULTS_DIR / "feature_correlation_matrix.png")
    print(f"‚úì Matriz de correlaci√≥n guardada")
    
    plot_feature_importance(consolidated_df, RESULTS_DIR / "feature_importance.png")
    print(f"‚úì Gr√°fico de importancia guardado")
    
    plot_time_series_comparison(df, TARGET_COL, exog_cols, RESULTS_DIR / "feature_time_series.png")
    print(f"‚úì Comparaci√≥n de series temporales guardada")
    
    print("\n" + "=" * 70)
    print("AN√ÅLISIS COMPLETADO")
    print("=" * 70)
    
    return consolidated_df


if __name__ == "__main__":
    results = main()
